{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm",
      "collapsed_sections": [
        "EJT_iv7FgdMG"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "EJT_iv7FgdMG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q bert_score transformers pandas numpy torch pydantic tqdm bitsandbytes"
      ],
      "metadata": {
        "id": "H7T4h-iBWnVi",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lNQz0lCVWMJx"
      },
      "outputs": [],
      "source": [
        "# from langchain_huggingface import HuggingFacePipeline\n",
        "# from langchain_core.prompts import PromptTemplate\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoModelForSequenceClassification, AutoTokenizer, pipeline, BitsAndBytesConfig, BertModel, BertTokenizer\n",
        "from bert_score import BERTScorer\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from typing import List, Any, Tuple\n",
        "import json\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "import logging\n",
        "import gc"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Functions"
      ],
      "metadata": {
        "id": "TDNZdnKjggVA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluation functions\n",
        "def calculate_bert(original: str, summary: str, scorer: BERTScorer) -> float:\n",
        "  '''This function serves as a relevance calculation between the two texts'''\n",
        "  _, __, f1 = scorer.score([original], [summary])\n",
        "  return f1\n",
        "\n",
        "def nli(m: AutoModelForSequenceClassification, t: AutoTokenizer, original: str, summary_sentences: List[str], hyperparameters: dict) -> bool:\n",
        "  '''This function serves as a measure of information faithfulness'''\n",
        "  claims = []\n",
        "  for summary_sentence in summary_sentences:\n",
        "    input_tokens = t(original, summary_sentence, return_tensors=\"pt\", truncation=True).to(m.device)\n",
        "    with torch.no_grad():\n",
        "      o = m(**input_tokens)\n",
        "      probs = torch.softmax(o.logits, dim=1)\n",
        "      index = np.dot(probs.to(\"cpu\"), np.array([0, 1, 2]))\n",
        "      claims.append(index)\n",
        "  return np.mean(claims)\n",
        "\n",
        "def simplicity(m: AutoModelForCausalLM, t: AutoTokenizer, sp: str, summary: str, hyperparameters: dict) -> bool:\n",
        "  '''This function serves as a measure of how simple the text is to understand'''\n",
        "  query = f\"\"\"\n",
        "  Evaluate the provided summary against the source text based on your assigned role.\n",
        "\n",
        "  Return your response ONLY as a JSON object that adheres strictly to the following schema.\n",
        "\n",
        "  ### JSON SCHEMA ###\n",
        "  ```json{{\n",
        "    \"simplicity_score\": \"INTEGER (1 to 5)\",\n",
        "    \"readability_critique\": \"STRING (A brief, two-sentence explanation of why the score was assigned, focusing on vocabulary, sentence length, and flow.)\",\n",
        "    \"most_confusing_term\": \"STRING (The single word or phrase that would be most challenging for your persona, or 'N/A' if none.)\"\n",
        "  }}```\n",
        "  ### END JSON SCHEMA ###\n",
        "\n",
        "  Summary:\n",
        "  ---\n",
        "  {summary}\n",
        "  ---\n",
        "  \"\"\"\n",
        "  o, i, __ = generate(m, t, query, hyperparameters, sp=sp)\n",
        "  return o, i"
      ],
      "metadata": {
        "id": "0o_pgUrIn1WT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# functions for generation\n",
        "def generate(model: AutoModelForCausalLM, tokenizer: AutoTokenizer, query: str, hyperparameters: dict, sp: str=None) -> Tuple[str, Any, str]:\n",
        "  messages = []\n",
        "  if sp != None:\n",
        "      messages.append({\n",
        "          \"role\": \"system\",\n",
        "          \"content\": sp\n",
        "          }\n",
        "      )\n",
        "  messages.append({\n",
        "      \"role\": \"user\",\n",
        "      \"content\": query\n",
        "  })\n",
        "\n",
        "\n",
        "  input_text = tokenizer.apply_chat_template(\n",
        "      messages,\n",
        "      tokenize=False,\n",
        "      add_generation_prompt=True\n",
        "  )\n",
        "  input_tokens = tokenizer.apply_chat_template(\n",
        "      messages,\n",
        "      tokenize=True,\n",
        "      add_generation_prompt=True,\n",
        "      return_tensors=\"pt\"\n",
        "  ).to(model.device)\n",
        "\n",
        "  with torch.no_grad():\n",
        "    output = model.generate(input_ids=input_tokens,\n",
        "                            return_dict_in_generate=True,\n",
        "                            output_scores=False,\n",
        "                            **hyperparameters)\n",
        "  return output, input_tokens, input_text\n",
        "\n",
        "\n",
        "def process_output(o, i, model_final_tag, model_final_tag_end, model_output_start, model_output_end):\n",
        "  output_no_input = tokenizer.decode(o.sequences[0][len(i[0]):])\n",
        "  output_no_thinking = output_no_input[output_no_input.index(model_final_tag)+len(model_final_tag):output_no_input.index(model_final_tag_end)]\n",
        "  output_no_spaces = output_no_thinking.replace(\"\\n\", \"\")\n",
        "  output_only_json = output_no_spaces[output_no_spaces.index(model_output_start)+len(model_output_start):output_no_spaces.rindex(model_output_end)]\n",
        "  example_output = json.loads(output_only_json)\n",
        "  return example_output"
      ],
      "metadata": {
        "id": "PuUQ0hPkqCHD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Configs"
      ],
      "metadata": {
        "id": "sCOTWi4qgjVw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# configs\n",
        "system_prompt = \"\"\"\n",
        "You are a highly reliable and expert Clinical Data Abstraction Agent specializing in natural language processing of Electronic Health Records (EHRs). Your primary goal is to transform unstructured clinical text into reliable, structured, and actionable data or patient-centric summaries, acting with the expertise of a medical writer and public health educator.\n",
        "\n",
        "RULES:\n",
        "1. Output Format (CRITICAL): The final and ONLY output must be a valid JSON object enclosed in ```json ... ``` (triple backticks). Do not include any text, reasoning, or verification output outside these fences.\n",
        "2. Model Role: Strictly adhere to the functional role of a Data Abstraction Agent and a Medical Writer/Educator.\n",
        "3. Traceability & Grounding: All extracted facts and summaries must be directly traceable to the provided clinical text. Do not invent or assume information. This rule is essential for liability mitigation.\n",
        "4. Jargon Control: Use the vocabulary and complexity level strictly defined in the <readability> field.\n",
        "5. Liability Mitigation: Before generating the final JSON, you must first output an internal verification step to ensure all CRITICAL actions are captured and correctly translated.\n",
        "6. Handling Ambiguity/Absence: If a required data point is completely absent or ambiguous, populate that field with \"N/A\" or \"Not Documented\". Do not use placeholder text from the schema (e.g., do not output \"[Medication Name]\").\n",
        "\"\"\"\n",
        "query_prompt = \"\"\"\n",
        "---TASK INSTRUCTION---\n",
        "Using the text provided in the <clinical_note> section to summarize the diagnoses and medications listed under <medications> and <diagnoses> sections, generate the required structured JSON output.\n",
        "\n",
        "**Target Directives (Safety First):**\n",
        "1. **Source Grounding & Verification (CRITICAL):** Before generating the final JSON, you **MUST** internally verify all facts and extracted actions against the source note. List all extracted actions and warning signs here, confirming their priority (CRITICAL, URGENT, ROUTINE).\n",
        "2. **Reading Level (CRITICAL):** Generate all fields using the <readbility> reading level.\n",
        "3. **Diagnosis Breakdown (CRITICAL):** Provide the diagnoses as defined in the schema. The <readbility> reading level MUST be strictly applied.\n",
        "4. **Medication Fidelity (CRITICAL):** All extracted medications must include the new `status` field. All medication fields (`med_name`, `regimen`, `status`, `purpose`) must be populated. The <readbility> reading level MUST be strictly applied to regimen and purpose medication fields.\n",
        "5. **Jargon Guardrails (CRITICAL):** The basic reading level MUST be strictly applied to the `warning_signs` array.\n",
        "\n",
        "---INTERNAL VERIFICATION---\n",
        "[LLM must insert the verification list here, per Directive 1]\n",
        "---END VERIFICATION---\n",
        "FINAL COMMAND: The JSON object MUST contain every CRITICAL action item listed in the verification step above. Failure to reconcile is grounds for system termination.\n",
        "\n",
        "---JSON SCHEMA DEFINITION---\n",
        "{{\n",
        "  \"diagnosis_list\": [\n",
        "    {{\n",
        "      \"original\": \"from input <diagnoses>\",\n",
        "      \"new\": \"rewritten based on <readability> level\"\n",
        "    }}\n",
        "  ],\n",
        "\n",
        "  \"medication_list\": [\n",
        "    {{\n",
        "      \"med_name\": \"[Medication Name]\",\n",
        "      \"regimen\": \"[Dose and Frequency]\",\n",
        "      \"status\": \"[New | Continued | Discontinued]\",\n",
        "      \"purpose\": \"[medication purpose]\",\n",
        "    }}\n",
        "  ],\n",
        "  \"follow_up_instructions\": [\n",
        "    {{\"action\": \"Schedule an appointment with [Primary Care Provider/Specialist]\", \"timeframe\": \"[e.g., within 7 days]\"}},\n",
        "    {{\"action\": \"Take all new medications as prescribed\", \"timeframe\": \"Ongoing\"}},\n",
        "    {{\"action\": \"Limit [Activity] and avoid [Food/Drug]\", \"timeframe\": \"[e.g., for 6 weeks]\"}}\n",
        "  ],\n",
        "  \"warning_signs\": [\n",
        "    \"[List of 3-5 signs that require a return to the ED or immediate call to the doctor, written in simple language]\"\n",
        "  ]\n",
        "}}\n",
        "---END SCHEMA---\n",
        "\n",
        "<readability>\n",
        "{}\n",
        "</readability>\n",
        "\n",
        "<clinical_note>\n",
        "{}\n",
        "</clinical_note>\n",
        "\n",
        "<diagnoses>\n",
        "{}\n",
        "</diagnoses>\n",
        "\n",
        "<medications>\n",
        "{}\n",
        "</medications>\n",
        "\"\"\"\n",
        "\n",
        "model_final_tag = \"<|end|><|start|>assistant<|channel|>final\"\n",
        "model_final_tag_end = \"<|return|>\"\n",
        "model_output_start = \"```json\"\n",
        "model_output_end = \"```\"\n",
        "basic = \"Simplest term, approx. 4.5-grade, e.g., 'Extra water (fluid) has built up inside your belly area.'\"\n",
        "intermediate = \"7th-grade term, e.g., 'A buildup of fluid in the space inside your abdomen or peritoneal cavity.'\"\n",
        "advanced = \"Specialist detail/MOA, e.g., 'Hepatic sinusoidal hypertension leads to splanchnic vasodilation and subsequent overflow due to RAAS activation.'\"\n",
        "output_eval_rules = \"RULES\\n1. Output Format (CRITICAL): The final and ONLY output must be a valid JSON object enclosed in ```json ... ``` (triple backticks). Do not include any text, reasoning, or verification output outside these fences.\"\n",
        "generation_model_name = \"openai/gpt-oss-20b\"\n",
        "evaluation_model_name = \"facebook/bart-large-mnli\"\n",
        "generation_configs = dict(max_new_tokens=5000, temperature=0.5, do_sample=True, top_k=10)\n",
        "evaluation_configs = dict(do_sample=False, max_new_tokens=5000)\n",
        "system_prompt_evaluation_basic = \"You are a 4.5 grade student who can understand simple terms. Please evaluate the medical summary (listed under Summary:) to see if you would understand this. Please return in the requested format.\"\n",
        "system_prompt_evaluation_intermediate = \"You are a 7th-grade student who can understand the following level of clincal language: A buildup of fluid in the space inside your abdomen or peritoneal cavity.'. Please evaluate the medical summary (listed under Summary:) to see if you would understand this.\"\n",
        "system_prompt_evaluation_advanced = \"You are a specialist (physician or researcher) with advanced clinical languange. Please evaluate the medical summary (listed under Summary:) to see if you would understand this.  Please return in the requested format.\""
      ],
      "metadata": {
        "id": "BKbRcUYym00o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Data"
      ],
      "metadata": {
        "id": "IrTmnfUPx4xB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num = 5"
      ],
      "metadata": {
        "id": "L2ge-M79sDt_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full_data = pd.read_csv(\"MIMIC-IV Notes Datathon v4 20251107(Sheet1).csv\", header=1)\n",
        "sampled_data = full_data.sample(29, random_state=0)\n",
        "sampled_data.loc[0,:] = full_data.iloc[0,:]\n",
        "data = sampled_data.iloc[-num:,:]\n",
        "data.reset_index(inplace=True)\n",
        "data.to_csv(\"sampled_data.csv\")"
      ],
      "metadata": {
        "id": "LR2mV7R7D4C-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate"
      ],
      "metadata": {
        "id": "U5SUt7AgglP3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logger = logging.getLogger(__name__)\n",
        "logger.setLevel(logging.DEBUG)\n",
        "file_handler = logging.FileHandler('app.log')\n",
        "file_handler.setLevel(logging.DEBUG) # Log all messages to the file\n",
        "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
        "\n",
        "# 5. Add the formatter to the handlers\n",
        "file_handler.setFormatter(formatter)\n",
        "logger.addHandler(file_handler)"
      ],
      "metadata": {
        "id": "Nd8h6y9So2Gl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(generation_model_name, device_map=\"auto\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(generation_model_name)"
      ],
      "metadata": {
        "id": "-zgBl0y_phER"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "processed_outputs = []"
      ],
      "metadata": {
        "id": "8WtKV4ybnWcB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for level in [basic, intermediate, advanced]:\n",
        "  for ind in tqdm(data.index, total=num):\n",
        "    example_query = query_prompt.format(level, data.loc[ind, \"Brief Hospital Course\"], data.loc[ind, \"Diagnosis List\"], data.loc[ind, \"Medication List\"])\n",
        "    start = time.perf_counter()\n",
        "    o, i, _ = generate(model, tokenizer, example_query, generation_configs, sp=system_prompt)\n",
        "    end = time.perf_counter()\n",
        "    try:\n",
        "      example_output = process_output(o, i, model_final_tag, model_final_tag_end, model_output_start, model_output_end)\n",
        "      processed_outputs.append([example_output, end-start])\n",
        "    except:\n",
        "      try:\n",
        "        example_output = process_output(o, i, model_final_tag, model_final_tag_end, \"\", \"\")\n",
        "        processed_outputs.append([example_output, end-start])\n",
        "      except Exception as e:\n",
        "        logger.error(f\"Error with processing output: {e}\")\n",
        "        processed_outputs.append(None)\n",
        "        pass\n",
        "    json.dump(processed_outputs, open(\"original_output.json\", \"w\"), indent=4)"
      ],
      "metadata": {
        "id": "KAd6XWU9rl-9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluate"
      ],
      "metadata": {
        "id": "cpXMM3STgoG_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"example_output.json\") as outfile:\n",
        "  processed_outputs = json.load(outfile)\n",
        "\n",
        "data = pd.read_csv(\"sampled_data.csv\")"
      ],
      "metadata": {
        "id": "xpxa_1wb5BTt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(generation_model_name, device_map=\"auto\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(generation_model_name)"
      ],
      "metadata": {
        "id": "o3fAwhaj1w7Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nli_model = AutoModelForSequenceClassification.from_pretrained(evaluation_model_name, device_map=\"auto\")\n",
        "nli_tokenizer = AutoTokenizer.from_pretrained(evaluation_model_name)\n",
        "label_dictionary = nli_model.config.id2label"
      ],
      "metadata": {
        "id": "-1uDs5wjJXHh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scorer = BERTScorer(model_type=\"bert-base-uncased\")"
      ],
      "metadata": {
        "id": "ZycEv7ju9nrX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_outputs = []\n",
        "for example_output in processed_outputs:\n",
        "  try:\n",
        "    unrolled = [json.dumps(value, indent=4) for value in example_output[0].values()]\n",
        "    all_outputs.append(unrolled)\n",
        "  except:\n",
        "    all_outputs.append(None)"
      ],
      "metadata": {
        "id": "19IWBNO3Oq1A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "relevance_scores = []"
      ],
      "metadata": {
        "id": "1q9WrMD51KuS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for ind, example_output in enumerate(processed_outputs):\n",
        "  try:\n",
        "    relevance = float(calculate_bert(data.loc[ind % num, \"Brief Hospital Course\"], json.dumps(example_output[0], indent=4), scorer))\n",
        "    relevance_scores.append(relevance)\n",
        "  except Exception as e:\n",
        "    print(e)\n",
        "    relevance_scores.append(None)"
      ],
      "metadata": {
        "id": "dQAdzz2ZK8Z1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "correctness_scores = []"
      ],
      "metadata": {
        "id": "lF-wQkbQ1BI1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for index1, example_output in enumerate(all_outputs):\n",
        "  try:\n",
        "    correctness = nli(nli_model, nli_tokenizer, data.loc[index1 % num, \"Brief Hospital Course\"], example_output, evaluation_configs)\n",
        "    correctness_scores.append(float(correctness))\n",
        "  except:\n",
        "    correctness_scores.append(None)"
      ],
      "metadata": {
        "id": "Xk50giHyjLIy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "simplicity_scores = []"
      ],
      "metadata": {
        "id": "KyZVThdd0yxL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for index2, example_output in tqdm(enumerate(processed_outputs), total=len(processed_outputs)):\n",
        "  eval_template = \"\"\n",
        "  if index2 < num:\n",
        "    eval_template = system_prompt_evaluation_basic\n",
        "  elif index2 >= num and index2 < num*2:\n",
        "    eval_template = system_prompt_evaluation_intermediate\n",
        "  else:\n",
        "    eval_template = system_prompt_evaluation_advanced\n",
        "  try:\n",
        "    if example_output == None:\n",
        "      simplicity_scores.append(None)\n",
        "      continue\n",
        "    simple, inp = simplicity(model, tokenizer, eval_template+\"\\n\"+output_eval_rules, json.dumps(example_output), evaluation_configs)\n",
        "    v = process_output(simple, inp, model_final_tag, model_final_tag_end, model_output_start, model_output_end)\n",
        "    simplicity_scores.append(v[\"simplicity_score\"])\n",
        "  except Exception as e:\n",
        "    print(e)\n",
        "    simplicity_scores.append(None)"
      ],
      "metadata": {
        "id": "rOmv5n33jNTg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Re-process for UI"
      ],
      "metadata": {
        "id": "fHIjsHK7OLUK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num = 1"
      ],
      "metadata": {
        "id": "VrDwAIgcKjY3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example = 0"
      ],
      "metadata": {
        "id": "fv1alTXKLN4F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "basic = processed_outputs[0:num][example][0]\n",
        "intermediate = processed_outputs[num:num*2][example][0]\n",
        "advanced = processed_outputs[num*2:][example][0]"
      ],
      "metadata": {
        "id": "UWJRw4NAgpTz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = {}\n",
        "keep = [\"regimen\", \"purpose\"]\n",
        "for b_dictionary, i_dictionary, a_dictionary in zip(basic[\"medication_list\"], intermediate[\"medication_list\"], advanced[\"medication_list\"]):\n",
        "  output[b_dictionary[\"med_name\"]] = {\"basic\": \"\\n\".join([value for key, value in b_dictionary.items() if key in keep]), \"intermediate\": \"\\n\".join([value for key, value in i_dictionary.items() if key in keep]), \"advanced\": \"\\n\".join([value for key, value in a_dictionary.items() if key in keep])}"
      ],
      "metadata": {
        "id": "Ha_uK8-ng-cU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for b_dictionary, i_dictionary, a_dictionary in zip(basic[\"diagnosis_list\"], intermediate[\"diagnosis_list\"], advanced[\"diagnosis_list\"]):\n",
        "  output[b_dictionary[\"original\"]] = {\"basic\": b_dictionary[\"new\"], \"intermediate\": i_dictionary[\"new\"], \"advanced\": a_dictionary[\"new\"]}"
      ],
      "metadata": {
        "id": "VyEaP6TKiouy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "json.dump(output, open(\"finalized_output.json\", \"w\"), indent=4)"
      ],
      "metadata": {
        "id": "qkaqm9mHhHrk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list_of_actions = []\n",
        "for item in basic[\"follow_up_instructions\"]:\n",
        "  list_of_actions.append(f\"{item['action']} - {item['timeframe']}\")"
      ],
      "metadata": {
        "id": "aHB59j6Pkk5s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "json.dump(list_of_actions, open(\"final_actions.json\", \"w\"))"
      ],
      "metadata": {
        "id": "SRDJLVzGlEPE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
